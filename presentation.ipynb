{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrective RAG (CRAG)\n",
    "\n",
    "An advanced RAG approach that **evaluates** retrieved documents and **takes corrective actions** to improve answer quality.\n",
    "\n",
    "**Problem with Traditional RAG:** It blindly uses all retrieved documents, even irrelevant ones, leading to poor answers or hallucinations.\n",
    "\n",
    "**CRAG Solution:** Evaluate document relevance first, then decide what action to take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Query → Embed → Retrieve → Evaluate → Corrective Action → Generate\n",
    "                              ↓\n",
    "                       [Relevance Score]\n",
    "                              ↓\n",
    "                 ┌───────────┼───────────┐\n",
    "                 ↓           ↓           ↓\n",
    "              CORRECT    AMBIGUOUS   INCORRECT\n",
    "              (≥0.5)     (0.3-0.5)    (≤0.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Document Loading & Chunking\n",
    "\n",
    "Large documents need to be split into smaller chunks for effective retrieval.\n",
    "\n",
    "**Strategy:** `RecursiveCharacterTextSplitter`\n",
    "- Tries to split on natural boundaries (paragraphs → sentences → words)\n",
    "- Maintains semantic coherence within chunks\n",
    "- Uses overlap to preserve context between chunks\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size=500`: Each chunk ~500 characters\n",
    "- `chunk_overlap=50`: 50 characters overlap between consecutive chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Priority order\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Embedding\n",
    "\n",
    "Convert text chunks into dense vector representations for semantic search.\n",
    "\n",
    "**Model:** `text-embedding-3-small` (via Aval AI API)\n",
    "- OpenAI's latest embedding model\n",
    "- 1536-dimensional vectors\n",
    "- Good balance between performance and cost\n",
    "\n",
    "**How it works:**\n",
    "- Text → Neural Network → Fixed-size vector\n",
    "- Similar texts have similar vectors (close in vector space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def embed_documents(texts):\n",
    "    response = requests.post(\n",
    "        \"https://api.avalai.ir/v1/embeddings\",\n",
    "        headers={\"Authorization\": f\"Bearer {API_KEY}\"},\n",
    "        json={\n",
    "            \"model\": \"text-embedding-3-small\",\n",
    "            \"input\": texts\n",
    "        }\n",
    "    )\n",
    "    return [item[\"embedding\"] for item in response.json()[\"data\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vector Store & Retrieval\n",
    "\n",
    "Store embeddings and retrieve relevant documents using similarity search.\n",
    "\n",
    "**Vector Database:** ChromaDB\n",
    "- Open-source, lightweight\n",
    "- Supports persistence to disk\n",
    "- Built-in similarity search\n",
    "\n",
    "**Retrieval Method:** Cosine Similarity\n",
    "- Finds k most similar documents to query\n",
    "- Returns documents ranked by similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Store embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Retrieve top-k similar documents\n",
    "docs = vectorstore.similarity_search(query, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Relevance Evaluation\n",
    "\n",
    "Score how relevant each document is to the query.\n",
    "\n",
    "**Model:** Cross-Encoder (`ms-marco-MiniLM-L-6-v2`)\n",
    "- Fine-tuned on MS MARCO (Question-Answering dataset)\n",
    "- ~22M parameters, fast inference\n",
    "\n",
    "**Why Cross-Encoder over Bi-Encoder?**\n",
    "\n",
    "| Bi-Encoder | Cross-Encoder |\n",
    "|------------|---------------|\n",
    "| Encodes query and doc separately | Encodes query + doc together |\n",
    "| Fast (can pre-compute) | Slower (must compute per pair) |\n",
    "| Less accurate | More accurate |\n",
    "\n",
    "Cross-Encoder sees the **interaction** between query and document, leading to better relevance judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")\n",
    "\n",
    "def score_relevance(query, document):\n",
    "    inputs = tokenizer(query, document, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    score = torch.sigmoid(outputs.logits).item()  # 0 to 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Path Determination\n",
    "\n",
    "Based on average relevance score, choose one of three corrective paths.\n",
    "\n",
    "**Thresholds:**\n",
    "- `≥ 0.5` → **Correct**: Documents are highly relevant\n",
    "- `≤ 0.3` → **Incorrect**: Documents are not relevant\n",
    "- `0.3 - 0.5` → **Ambiguous**: Uncertain, need additional sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_path(scored_docs, threshold_correct=0.5, threshold_incorrect=0.3):\n",
    "    avg_score = sum(score for _, score in scored_docs) / len(scored_docs)\n",
    "    \n",
    "    if avg_score >= threshold_correct:\n",
    "        return \"correct\"\n",
    "    elif avg_score <= threshold_incorrect:\n",
    "        return \"incorrect\"\n",
    "    else:\n",
    "        return \"ambiguous\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Corrective Actions\n",
    "\n",
    "### 6.1 Correct Path (score ≥ 0.5)\n",
    "\n",
    "Documents are relevant → **Decompose, Filter, Recompose**\n",
    "\n",
    "- Remove documents with individual score < 0.3\n",
    "- Keep only high-quality, relevant documents\n",
    "- Use filtered documents for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_path(scored_docs, min_score=0.3):\n",
    "    # Filter out low-relevance documents\n",
    "    filtered = [doc for doc, score in scored_docs if score >= min_score]\n",
    "    return filtered, \"Retrieved Documents (Filtered)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Incorrect Path (score ≤ 0.3)\n",
    "\n",
    "Documents are not relevant → **Web Search Fallback**\n",
    "\n",
    "- Discard all retrieved documents\n",
    "- Search the web for fresh information\n",
    "- Use web results for generation\n",
    "\n",
    "**Search Engine:** DuckDuckGo (no API key required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def incorrect_path(query, max_results=3):\n",
    "    results = DDGS().text(query, max_results=max_results)\n",
    "    web_docs = [\n",
    "        Document(page_content=r[\"body\"], metadata={\"source\": r[\"href\"]})\n",
    "        for r in results\n",
    "    ]\n",
    "    return web_docs, \"Web Search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Ambiguous Path (0.3 < score < 0.5)\n",
    "\n",
    "Uncertain relevance → **Hybrid Approach**\n",
    "\n",
    "- Keep filtered documents (some might be useful)\n",
    "- Also search the web for additional context\n",
    "- Combine both sources for generation\n",
    "\n",
    "This provides the best of both worlds when confidence is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguous_path(query, scored_docs):\n",
    "    # Get filtered documents\n",
    "    filtered_docs, _ = correct_path(scored_docs)\n",
    "    # Get web results\n",
    "    web_docs, _ = incorrect_path(query, max_results=2)\n",
    "    # Combine both\n",
    "    return filtered_docs + web_docs, \"Hybrid (Documents + Web)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Answer Generation\n",
    "\n",
    "Generate final answer using LLM with selected documents as context.\n",
    "\n",
    "**Model:** `gpt-4o-mini` (via Aval AI API)\n",
    "- Fast and cost-effective\n",
    "- Good at following instructions\n",
    "- Handles context well\n",
    "\n",
    "**Prompt includes:**\n",
    "- Context from selected documents\n",
    "- Knowledge source (so LLM knows where info came from)\n",
    "- User's question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=AVALAI_API_KEY,\n",
    "    openai_api_base=\"https://api.avalai.ir/v1\"\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context (from {knowledge_source}):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based on the context. If context is insufficient, say so.\n",
    "\"\"\"\n",
    "\n",
    "answer = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Streamlit Interface\n\nA web-based chat interface for interacting with PDFs.\n\n**Features:**\n- PDF upload and automatic processing\n- Real-time chat with document\n- Displays which path was taken (Correct/Incorrect/Ambiguous)\n- Shows relevance scores and knowledge source\n\n**Components:**\n- `st.file_uploader`: Upload PDF files\n- `st.chat_input`: User question input\n- `st.chat_message`: Display conversation\n- Session state: Maintain chat history",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\n| Step | Component | Technology | Purpose |\n|------|-----------|------------|--------|\n| 1 | Chunking | RecursiveCharacterTextSplitter | Split docs into semantic chunks |\n| 2 | Embedding | text-embedding-3-small | Convert text to vectors |\n| 3 | Storage | ChromaDB | Store and search vectors |\n| 4 | Evaluation | Cross-Encoder (MiniLM) | Score query-doc relevance |\n| 5 | Path Selection | Threshold-based | Choose corrective action |\n| 6 | Correction | Filter / Web / Hybrid | Get best context |\n| 7 | Generation | gpt-4o-mini | Generate final answer |\n| 8 | Interface | Streamlit | Web-based chat UI |\n\n**Key Benefits:**\n- Reduces hallucination from irrelevant context\n- Falls back to web when knowledge base fails\n- Adaptive strategy based on confidence\n\n**Run the app:**\n```bash\nstreamlit run app.py\n```",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n| Step | Component | Technology | Purpose |\n|------|-----------|------------|--------|\n| 1 | Chunking | RecursiveCharacterTextSplitter | Split docs into semantic chunks |\n| 2 | Embedding | text-embedding-3-small | Convert text to vectors |\n| 3 | Storage | ChromaDB | Store and search vectors |\n| 4 | Evaluation | Cross-Encoder (MiniLM) | Score query-doc relevance |\n| 5 | Path Selection | Threshold-based | Choose corrective action |\n| 6 | Correction | Filter / Web / Hybrid | Get best context |\n| 7 | Generation | gpt-4o-mini | Generate final answer |\n| 8 | Interface | Streamlit | Web-based chat UI |\n\n**Key Benefits:**\n- Reduces hallucination from irrelevant context\n- Falls back to web when knowledge base fails\n- Adaptive strategy based on confidence\n\n**Run the app:**\n```bash\nstreamlit run app.py\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}