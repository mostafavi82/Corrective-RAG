{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Corrective RAG (CRAG)\n\nAn advanced RAG approach that **evaluates** retrieved documents and **takes corrective actions** to improve answer quality.\n\n![CRAG Architecture](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*qKV_BQ4X2cFVhU1DIMRtKw.png)\n\n**Problem with Traditional RAG:** It blindly uses all retrieved documents, even irrelevant ones, leading to poor answers or hallucinations.\n\n**CRAG Solution:** Evaluate document relevance first using LLM, then decide what action to take."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Architecture Overview\n\n```\nQuery -> Embed (Metis) -> Retrieve (ChromaDB) -> Evaluate (LLM) -> Corrective Action -> Generate\n                                                       |\n                                                [Relevance Check]\n                                                       |\n                                          +------------+------------+\n                                          |            |            |\n                                       CORRECT     AMBIGUOUS    INCORRECT\n                                       (>=50%)     (30-50%)      (<=30%)\n                                          |            |            |\n                                       Filter     Filter+Web    Web Search\n                                                                 (Tavily)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Document Loading & Chunking\n",
    "\n",
    "Large documents need to be split into smaller chunks for effective retrieval.\n",
    "\n",
    "**Strategy:** `RecursiveCharacterTextSplitter`\n",
    "- Tries to split on natural boundaries (paragraphs -> sentences -> words)\n",
    "- Maintains semantic coherence within chunks\n",
    "- Uses overlap to preserve context between chunks\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size=500`: Each chunk ~500 characters\n",
    "- `chunk_overlap=50`: 50 characters overlap between consecutive chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Priority order\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Embedding\n\nConvert text chunks into dense vector representations for semantic search.\n\n**Model:** `text-embedding-3-small` (via Metis AI API)\n- OpenAI's embedding model\n- 1536-dimensional vectors\n- Good balance between performance and cost\n\n**How it works:**\n- Text -> Neural Network -> Fixed-size vector\n- Similar texts have similar vectors (close in vector space)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import requests\n\ndef embed_documents(texts):\n    response = requests.post(\n        \"https://api.metisai.ir/openai/v1/embeddings\",\n        headers={\"Authorization\": f\"Bearer {METIS_API_KEY}\"},\n        json={\n            \"model\": \"text-embedding-3-small\",\n            \"input\": texts\n        }\n    )\n    return [item[\"embedding\"] for item in response.json()[\"data\"]]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vector Store & Retrieval\n",
    "\n",
    "Store embeddings and retrieve relevant documents using similarity search.\n",
    "\n",
    "**Vector Database:** ChromaDB\n",
    "- Open-source, lightweight\n",
    "- Supports persistence to disk\n",
    "- Built-in similarity search\n",
    "\n",
    "**Retrieval Method:** Cosine Similarity\n",
    "- Finds k most similar documents to query\n",
    "- Returns documents ranked by similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Store embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Retrieve top-k similar documents\n",
    "docs = vectorstore.similarity_search(query, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: LLM-based Relevance Evaluation\n\nEvaluate how relevant each retrieved document is to the query using an LLM.\n\n**Model:** `qwen2.5-vl-3b-instruct` (via Aval AI API)\n- Lightweight and fast\n- Good at following instructions\n- Batch evaluation (all documents at once)\n\n**Why LLM over traditional methods?**\n\n| Traditional (Bi-Encoder) | LLM Evaluation |\n|--------------------------|----------------|\n| Limited semantic understanding | Deep semantic understanding |\n| Can't handle complex queries | Understands nuanced queries |\n| Requires fine-tuning | Works out of the box |\n\n**Output:** For each document: `yes` (relevant) or `no` (not relevant)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nllm = ChatOpenAI(\n    model=\"qwen2.5-vl-3b-instruct\",\n    temperature=0,\n    openai_api_key=AVALAI_API_KEY,\n    openai_api_base=\"https://api.avalai.ir/v1\"\n)\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"human\", \"\"\"Check which documents are relevant to the question.\n\nQuestion: {question}\n\nDocuments:\n{documents}\n\nAnswer 'yes' or 'no' for each document. Format: yes, no, yes, no\"\"\")\n])\n\ngrader = prompt | llm | StrOutputParser()\n\n# Example: \"no, no, yes, no\" -> [0.0, 0.0, 1.0, 0.0]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Path Determination\n\nBased on **relevance ratio** (% of relevant documents), choose one of three corrective paths.\n\n**Thresholds:**\n- `>= 50%` relevant -> **Correct**: Most documents are useful\n- `<= 30%` relevant -> **Incorrect**: Documents are not relevant  \n- `30% - 50%` -> **Ambiguous**: Uncertain, need additional sources"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def determine_path(scores, threshold_correct=0.5, threshold_incorrect=0.3):\n    relevant_count = sum(1 for s in scores if s >= 0.5)\n    relevance_ratio = relevant_count / len(scores)\n    \n    if relevance_ratio >= threshold_correct:\n        return \"correct\"\n    elif relevance_ratio <= threshold_incorrect:\n        return \"incorrect\"\n    else:\n        return \"ambiguous\"\n\n# Example: scores = [0.0, 0.0, 1.0, 0.0]\n# relevance_ratio = 1/4 = 0.25 -> \"incorrect\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Corrective Actions\n",
    "\n",
    "### 6.1 Correct Path (score >= 0.5)\n",
    "\n",
    "Documents are relevant -> **Decompose, Filter, Recompose**\n",
    "\n",
    "- Remove documents with individual score < 0.3\n",
    "- Keep only high-quality, relevant documents\n",
    "- Use filtered documents for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_path(scored_docs, min_score=0.3):\n",
    "    # Filter out low-relevance documents\n",
    "    filtered = [doc for doc, score in scored_docs if score >= min_score]\n",
    "    return filtered, \"Retrieved Documents (Filtered)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.2 Incorrect Path (relevance <= 30%)\n\nDocuments are not relevant -> **Web Search Fallback**\n\n- Discard all retrieved documents\n- Rewrite question for better web search\n- Search the web using Tavily API\n- Use web results for generation\n\n**Search Engine:** Tavily (fast, accurate, designed for LLM apps)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.documents import Document\n\n# Question rewriter for better web search\nrewriter = ChatPromptTemplate.from_messages([\n    (\"human\", \"Rewrite this question for web search:\\n\\n{question}\")\n]) | llm | StrOutputParser()\n\n# Tavily search\nsearch_tool = TavilySearchResults(k=3)\n\ndef incorrect_path(query, max_results=3):\n    # Rewrite question\n    rewritten = rewriter.invoke({\"question\": query})\n    \n    # Search web\n    results = search_tool.invoke({\"query\": rewritten})\n    \n    web_docs = [\n        Document(page_content=r[\"content\"], metadata={\"url\": r[\"url\"]})\n        for r in results[:max_results]\n    ]\n    return web_docs, \"Web Search (Tavily)\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.3 Ambiguous Path (30% < relevance < 50%)\n\nUncertain relevance -> **Hybrid Approach**\n\n- Keep filtered documents (some might be useful)\n- Also search the web for additional context\n- Combine both sources for generation\n\nThis provides the best of both worlds when confidence is low."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguous_path(query, scored_docs):\n",
    "    # Get filtered documents\n",
    "    filtered_docs, _ = correct_path(scored_docs)\n",
    "    # Get web results\n",
    "    web_docs, _ = incorrect_path(query, max_results=2)\n",
    "    # Combine both\n",
    "    return filtered_docs + web_docs, \"Hybrid (Documents + Web)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Answer Generation\n",
    "\n",
    "Generate final answer using LLM with selected documents as context.\n",
    "\n",
    "**Model:** `gpt-4o-mini` (via Aval AI API)\n",
    "- Fast and cost-effective\n",
    "- Good at following instructions\n",
    "- Handles context well\n",
    "\n",
    "**Prompt includes:**\n",
    "- Context from selected documents\n",
    "- Knowledge source (so LLM knows where info came from)\n",
    "- User's question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=AVALAI_API_KEY,\n",
    "    openai_api_base=\"https://api.avalai.ir/v1\"\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context (from {knowledge_source}):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based on the context. If context is insufficient, say so.\n",
    "\"\"\"\n",
    "\n",
    "answer = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Streamlit Interface\n",
    "\n",
    "A web-based chat interface for interacting with PDFs.\n",
    "\n",
    "**Features:**\n",
    "- PDF upload and automatic processing\n",
    "- Real-time chat with document\n",
    "- Displays which path was taken (Correct/Incorrect/Ambiguous)\n",
    "- Shows relevance scores and knowledge source\n",
    "\n",
    "**Components:**\n",
    "- `st.file_uploader`: Upload PDF files\n",
    "- `st.chat_input`: User question input\n",
    "- `st.chat_message`: Display conversation\n",
    "- Session state: Maintain chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF Upload\n",
    "pdf_file = st.file_uploader(\"Upload PDF\", type=['pdf'])\n",
    "\n",
    "if pdf_file:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load()\n",
    "    vector_store.load_documents([p.page_content for p in pages])\n",
    "\n",
    "# Chat interface\n",
    "if prompt := st.chat_input(\"Ask a question...\"):\n",
    "    result = rag.query(prompt, return_metadata=True)\n",
    "    \n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "    st.chat_message(\"assistant\").write(result[\"answer\"])\n",
    "    \n",
    "    # Show path info\n",
    "    st.info(f\"Path: {result['path_type']} | Score: {result['avg_relevance_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n| Step | Component | Technology | Provider |\n|------|-----------|------------|----------|\n| 1 | Chunking | RecursiveCharacterTextSplitter | LangChain |\n| 2 | Embedding | text-embedding-3-small | Metis AI |\n| 3 | Storage | ChromaDB | Local |\n| 4 | Evaluation | qwen2.5-vl-3b-instruct (LLM) | Aval AI |\n| 5 | Path Selection | Threshold-based | - |\n| 6 | Web Search | Tavily + Question Rewriter | Tavily + Metis AI |\n| 7 | Generation | gpt-4o-mini | Aval AI |\n| 8 | Interface | Streamlit | - |\n\n**Key Benefits:**\n- Reduces hallucination from irrelevant context\n- Falls back to web when knowledge base fails\n- Adaptive strategy based on confidence\n- Batch evaluation (single LLM call for all docs)\n\n**Run the app:**\n```bash\nstreamlit run app.py\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}